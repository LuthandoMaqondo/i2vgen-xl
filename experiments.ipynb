{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/LuthandoMaqondo/i2vgen-xl/blob/luthando-contribution/experiments.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    WORKING_DIR = '.'\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    WORKING_DIR = '/content/drive/MyDrive/Colab Notebooks'\n",
    "    # Mount drive in order access Google drive\n",
    "    drive.mount('/content/drive',  force_remount=True)\n",
    "    \n",
    "if IN_COLAB:\n",
    "    sys.path.insert(0, WORKING_DIR)\n",
    "else:\n",
    "    # The actual code is one level higher in folder depth/structure, so we're elevating this notebook.\n",
    "    sys.path.insert(0,f\".{WORKING_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if IN_COLAB:\n",
    "!git clone https://github.com/LuthandoMaqondo/i2vgen-xl.git\n",
    "%cd i2vgen-xl\n",
    "!git checkout luthando-contribution\n",
    "!ls\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !wget https://huggingface.co/ali-vilab/modelscope-damo-text-to-video-synthesis/raw/main/open_clip_pytorch_model.bin\n",
    "# !git clone https://huggingface.co/damo-vilab/text-to-video-ms-1.7b ./models/model_scope_diffusers/\n",
    "!git clone https://huggingface.co/damo-vilab/text-to-video-ms-1.7b ./models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ./configs/t2v_train_custom.yaml\n",
    "\n",
    "# TASK_TYPE: train_t2v_entrance\n",
    "# ENABLE: true\n",
    "# use_ema: false\n",
    "# num_workers: 6\n",
    "# frame_lens: [1, 16, 16, 16, 16, 32, 32, 32]\n",
    "# sample_fps: [1,  8,  16, 16, 16, 8,  16, 16]\n",
    "# resolution: [448, 256]\n",
    "# vit_resolution: [224, 224]\n",
    "# vid_dataset: {\n",
    "#     'type': 'VideoDataset',\n",
    "#     'data_list': ['data/vid_list.txt', ],\n",
    "#     'data_dir_list': ['data/videos/', ],\n",
    "#     'vit_resolution': [224, 224],\n",
    "#     'resolution': [448, 256],\n",
    "#     'get_first_frame': True,\n",
    "#     'max_words': 1000,\n",
    "# }\n",
    "# img_dataset: {\n",
    "#     'type': 'ImageDataset',\n",
    "#     'data_list': ['data/img_list.txt', ],\n",
    "#     'data_dir_list': ['data/images', ],\n",
    "#     'vit_resolution': [224, 224],\n",
    "#     'resolution': [448, 256],\n",
    "#     'max_words': 1000\n",
    "# }\n",
    "# embedder: {\n",
    "#     'type': 'FrozenOpenCLIPTtxtVisualEmbedder',\n",
    "#     'layer': 'penultimate',\n",
    "#     'vit_resolution': [224, 224],\n",
    "#     'pretrained': 'models/open_clip_pytorch_model.bin'\n",
    "# }\n",
    "# UNet: {\n",
    "#     'type': 'UNetSD_T2VBase',\n",
    "#     'in_dim': 4,\n",
    "#     'y_dim': 1024,\n",
    "#     'upper_len': 128,\n",
    "#     'context_dim': 1024,\n",
    "#     'out_dim': 4,\n",
    "#     'dim_mult': [1, 2, 4, 4],\n",
    "#     'num_heads': 8,\n",
    "#     'default_fps': 8,\n",
    "#     'head_dim': 64,\n",
    "#     'num_res_blocks': 2,\n",
    "#     'dropout': 0.1,\n",
    "#     'misc_dropout': 0.4,\n",
    "#     'temporal_attention': True,\n",
    "#     'temporal_attn_times': 1,\n",
    "#     'use_checkpoint': True,\n",
    "#     'use_fps_condition': False,\n",
    "#     'use_sim_mask': False\n",
    "# }\n",
    "# Diffusion: {\n",
    "#     'type': 'DiffusionDDIM',\n",
    "#     'schedule': 'linear_sd', # cosine\n",
    "#     'schedule_param': {\n",
    "#         'num_timesteps': 1000,\n",
    "#         'init_beta': 0.00085,\n",
    "#         'last_beta': 0.0120,\n",
    "#         'zero_terminal_snr': False,\n",
    "#     },\n",
    "#     'mean_type': 'eps',\n",
    "#     'loss_type': 'mse',\n",
    "#     'var_type': 'fixed_small',\n",
    "#     'rescale_timesteps': False,\n",
    "#     'noise_strength': 0.0\n",
    "# }\n",
    "# batch_sizes: {\n",
    "#     \"1\": 32,\n",
    "#     \"4\": 8,\n",
    "#     \"8\": 4,\n",
    "#     \"16\": 4,\n",
    "#     \"32\": 2\n",
    "# }\n",
    "# visual_train: {\n",
    "#     'type': 'VisualTrainTextImageToVideo',\n",
    "#     'partial_keys': [\n",
    "#         ['y', 'fps'],\n",
    "#     ],\n",
    "#     'use_offset_noise': False,\n",
    "#     'guide_scale': 9.0, \n",
    "# }\n",
    "\n",
    "# Pretrain: {\n",
    "#     'type': pretrain_specific_strategies,\n",
    "#     'fix_weight': False,\n",
    "#     'grad_scale': 0.5,\n",
    "#     # # 'resume_checkpoint': 'workspace/model_bk/model_scope_0267000.pth',\n",
    "#     # 'resume_checkpoint': 'workspace/model_bk/text2video_pytorch_model.pth',\n",
    "#     'resume_checkpoint': 'workspace/model_bk/text2video_pytorch_model_0267000.pth',\n",
    "#     'sd_keys_path': 'data/stable_diffusion_image_key_temporal_attention_x1.json',\n",
    "# }\n",
    "\n",
    "# chunk_size: 4\n",
    "# decoder_bs: 4\n",
    "# lr: 0.00003\n",
    "\n",
    "# noise_strength: 0.1\n",
    "# # classifier-free guidance\n",
    "# p_zero: 0.1\n",
    "# guide_scale: 3.0\n",
    "# num_steps: 100\n",
    "\n",
    "# use_zero_infer: True\n",
    "# viz_interval: 5        # 200\n",
    "# save_ckp_interval: 50   # 500\n",
    "\n",
    "# # Log\n",
    "# log_dir: \"workspace/experiments\"\n",
    "# log_interval: 1\n",
    "# seed: 8888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing STARTED!\n",
      "Data preprocessing DONE: 34 text prompts -> 34 videos\n"
     ]
    }
   ],
   "source": [
    "!python preprocess.py \\\n",
    "    --json_file_path \"{WORKING_DIR if IN_COLAB else '/home/luthando/.cache'}/datasets/Appimate/dataset.json\" \\\n",
    "    --output_dir \"{WORKING_DIR if IN_COLAB else '/home/luthando/.cache'}/datasets/Appimate/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/luthando/miniconda3/envs/vgen/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.2.0+cu121 with CUDA 1201 (you have 2.2.0)\n",
      "    Python  3.8.18 (you have 3.8.18)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "!python train_net.py --cfg configs/t2v_train_custom.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python inference.py --cfg configs/t2v_infer.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
